\section{Mandatory Assignment Answers}
\label{sec:mandatory_answers}

\subsection{Supervised Learning and Business Applications}
\textbf{Q1: Supervised Learning Definition}\\
Supervised learning is a data analysis paradigm used to extract models that describe important data classes or forecast future trends. It relies on a labeled dataset where the algorithm learns from known pairs of inputs and outputs. 
\begin{itemize}
    \item \textbf{Classification:} Predicts categorical class labels (e.g., "safe" or "risky").
    \item \textbf{Prediction (Regression):} Forecasts continuous-valued functions or numeric values.
\end{itemize}

\textbf{Q2: Italian Clothing Company Case}\\
In the business context of product demand, a clothing brand used linear regression to quantify the relationship between advertising spend and sales. The resulting model was:
\begin{equation}
    Sales = 168 + 23 \times (Advertising)
\end{equation}

\subsection{Prediction Techniques and Time-Series Analysis}
\textbf{Q3: Popular Prediction Techniques}\\
\begin{itemize}
    \item \textbf{Linear Regression:} Minimizes error in $Y = \beta_0 + \beta_1x$.
    \item \textbf{Neural Networks:} Uses \textbf{Forward Propagation} for activations and \textbf{Back Propagation} to adjust weights based on errors.
    \item \textbf{Support Vector Machine (SVM):} Creates a hyperplane to separate data in high-dimensional space.
    \item \textbf{Random Forests:} An ensemble of decision trees that improves robustness by averaging multiple results.
\end{itemize}

\textbf{Q4-Q5: Time-Series Methodology}\\
Time-series analysis focuses on predicting future values based solely on previously observed values. \textbf{Decomposition} splits data into:
\begin{itemize}
    \item \textbf{Seasonal:} Repeating patterns within a fixed period.
    \item \textbf{Trend:} Long-term underlying direction.
    \item \textbf{Noise:} Random residuals after removing trend and seasonality.
\end{itemize}
Techniques like \textbf{Holt-Winters} handle systematic trends and seasonality, while \textbf{ARIMA(p, d, q)} uses autoregressive terms and moving averages.

\subsection[Performance Metrics]{Performance Metrics and Historical Benchmarks}
\textbf{Q6: Measuring Quality with MAPE}\\
Mean Absolute Percentage Error (MAPE) is the average of absolute percentage errors. Absolute values are used so that positive and negative errors do not cancel each other out, providing an honest accuracy metric.

\subsection[Beer Dataset]{The Australian Beer Dataset Case Study (Q7)}
Analysis of monthly Australian beer consumption (1956--1995) demonstrates how \textbf{Time Series Decomposition} converts complex fluctuations into predictable patterns. By isolating the \textbf{Trend} (long-term growth), \textbf{Seasonal} (repeating annual peaks), and \textbf{Noise} components, demand forecasting becomes statistically grounded.

Modeling benchmarks showed that \textbf{Holt-Winters} effectively captured these cycles, yielding a highly accurate \textbf{MAPE of 5.44\%}, while the \textbf{ARIMA(4, 0, 0)} model leveraged four lags for its autoregressive estimations. Ultimately, this analysis proves that brewery consumption is cyclically structured rather than random, enabling optimized 12-month inventory and production planning.

\subsection[Model Experiments]{Custom Comparative Findings: Electricity Demand (Q8)}
Our experimental results on high-frequency electricity consumption data revealed a significant performance gap between classical and modern ensemble methods. Based on our final evaluations:
\begin{itemize}
	\item \textbf{Deep Learning vs. Kernels:} The \textbf{NN (3-Layer)} achieved a highly competitive \textbf{10.76\% MAPE}, proving that multi-layer connections can effective map complex load surges. However, it required significant computational overhead (~39s).
	\item \textbf{Classical Baseline:} Linear Regression, despite its speed, lagged behind with a \textbf{11.49\% MAPE}, failing to capture the non-linear intraday peaks.
	\item \textbf{Modern Ensemble Performance:} \textbf{LightGBM} emerged as the definitive benchmark leader with a \textbf{9.88\% MAPE}. This demonstrates that for high-resolution datasets, gradient boosting on discretized histograms outperforms both classical ARIMA (which struggled with a 63.36\% error) and deep neural architectures.
\end{itemize}
\textbf{Conclusion:} The choice of architecture is the primary driver of accuracy. While classical models are efficient, they lack the "contextual depth" required to handle the irregular spikes inherent in real-world numeric datasets.rd kernels.