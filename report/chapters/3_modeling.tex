\section{Hyperparameter Tuning and Optimization}
\label{sec:modeling}

\subsection{Optuna-Based Bayesian Search}
To move beyond manual heuristic selection, we employed \textbf{Optuna}, an automated hyperparameter optimization framework. We executed a study consisting of \textbf{100 trials}, utilizing a Tree-structured Parzen Estimator (TPE) to minimize the Mean Absolute Percentage Error (MAPE). 

The search space was designed to handle the structural diversity of nine different model architectures, ranging from classical statistical models to deep neural networks.

\subsection{Model-Specific Search Spaces}
The optimization process focused on the following key parameters:
\begin{itemize}
    \item \textbf{Statistical Models (ARIMA):} We explored the $(p, d, q)$ order space in the range $[0, 3]$ for $p, q$ and $[0, 1]$ for $d$, allowing the model to adapt to various degrees of trend and autocorrelation.
    \item \textbf{Kernel Methods (SVR):} We tuned the regularization parameter $C \in [0.1, 10.0]$ across both \textit{linear} and \textit{RBF} kernels to balance margin violations with boundary smoothness.
    \item \textbf{Tree-Based (XGBoost/LightGBM):} While using 500 estimators as a constant, we leveraged GPU-accelerated histogram methods (\texttt{tree\_method='hist'}) to handle the large feature space efficiently.
    \item \textbf{Neural Networks:} For both the 1-Layer and 3-Layer architectures, we optimized the number of units per layer between 32 and 256, ensuring the network had sufficient capacity to map intra-day demand patterns without over-parameterization.
\end{itemize}

\subsection{Tuning Stability and Best Trial}
As documented in our \texttt{optuna\_trials\_final.csv}, the study successfully converged on parameters that reduced MAPE significantly from the initial baseline levels. For the final experimental results, each model was retrained using the specific parameters from its respective "Best Trial" to ensure a fair comparison. 

The \textbf{ARIMA(0,0,3)} and \textbf{NN (3-Layer with [117, 173, 247] units)} emerged as high-performing configurations from the parameter search, which were then validated on the unseen test set in the following chapter.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/tuning_results_2.png}
    \caption{Optuna objective expansion showing the convergence toward minimal MAPE across 100 trials.}
    \label{fig:optuna_convergence}
\end{figure}
